#!/usr/bin/env python3
"""
RMSAI Improvements Test Suite
=============================

Comprehensive test suite for all four improvement modules:
1. Streaming API
2. Advanced Analytics
3. Adaptive Thresholds
4. Monitoring Dashboard

Usage:
    python test_improvements.py [module]

    # Test all improvements
    python test_improvements.py

    # Test specific module
    python test_improvements.py api
    python test_improvements.py analytics
    python test_improvements.py thresholds
    python test_improvements.py classification
    python test_improvements.py dashboard
    python test_improvements.py patient_analysis
    python test_improvements.py pacer
    python test_improvements.py processor
"""

import sys
import time
import subprocess
import requests
import json
import logging
from pathlib import Path
from datetime import datetime
import threading
from typing import Dict

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ImprovementTester:
    """Test suite for RMSAI improvements"""

    def __init__(self):
        self.results = {}
        self.processes = {}

    def test_api_server(self) -> bool:
        """Test the streaming API server"""
        logger.info("Testing Streaming API Server...")

        try:
            # Start API server in background
            logger.info("Starting API server...")
            process = subprocess.Popen([
                sys.executable, "api_server.py"
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            self.processes['api'] = process

            # Wait for server to start
            time.sleep(5)

            # Test health endpoint
            response = requests.get("http://localhost:8000/health", timeout=5)
            if response.status_code != 200:
                logger.error("Health check failed")
                return False

            health_data = response.json()
            logger.info(f"Health check passed: {health_data['status']}")

            # Test stats endpoint
            response = requests.get("http://localhost:8000/api/v1/stats", timeout=5)
            if response.status_code == 200:
                stats = response.json()
                logger.info(f"Stats endpoint working: {stats['total_chunks']} chunks")
            else:
                logger.warning("Stats endpoint not available (no data)")

            # Test anomalies endpoint
            response = requests.post(
                "http://localhost:8000/api/v1/anomalies",
                json={"limit": 5},
                timeout=5
            )
            if response.status_code == 200:
                anomalies = response.json()
                logger.info(f"Anomalies endpoint working: {anomalies['count']} anomalies")
            else:
                logger.warning("Anomalies endpoint returned error (expected if no data)")

            # Test conditions endpoint
            response = requests.get("http://localhost:8000/api/v1/conditions", timeout=5)
            if response.status_code == 200:
                conditions = response.json()
                logger.info(f"Conditions endpoint working: {conditions['total_conditions']} conditions")

            # Test leads endpoint
            response = requests.get("http://localhost:8000/api/v1/leads", timeout=5)
            if response.status_code == 200:
                leads = response.json()
                logger.info(f"Leads endpoint working: {leads['total_leads']} leads")

            # Test WebSocket test page
            response = requests.get("http://localhost:8000/test-websocket", timeout=5)
            if response.status_code == 200:
                logger.info("WebSocket test page accessible")

            # Test API documentation
            response = requests.get("http://localhost:8000/docs", timeout=5)
            if response.status_code == 200:
                logger.info("API documentation accessible")

            # Test pacer analysis endpoints
            response = requests.get("http://localhost:8000/api/v1/pacer-analysis", timeout=10)
            if response.status_code == 200:
                pacer_data = response.json()
                logger.info(f"Pacer analysis endpoint working: {pacer_data.get('pacer_data_available', False)}")
                if pacer_data.get('pacer_data_available'):
                    logger.info(f"  - Found {pacer_data.get('total_chunks_with_pacer_data', 0)} chunks with pacer data")
                    logger.info(f"  - Pacer types: {list(pacer_data.get('pacer_type_distribution', {}).get('counts', {}).keys())}")
            else:
                logger.warning("Pacer analysis endpoint returned error (expected if no pacer data)")

            # Test advanced pacer analytics endpoint
            response = requests.get("http://localhost:8000/api/v1/analytics/pacer-patterns", timeout=10)
            if response.status_code == 200:
                pacer_analytics = response.json()
                logger.info("Advanced pacer analytics endpoint working")
                if 'results' in pacer_analytics and 'total_chunks_analyzed' in pacer_analytics['results']:
                    logger.info(f"  - Analyzed {pacer_analytics['results']['total_chunks_analyzed']} chunks")
            else:
                logger.warning("Advanced pacer analytics endpoint not available")

            # Test pacer threshold impact endpoint
            response = requests.get("http://localhost:8000/api/v1/thresholds/pacer-impact", timeout=10)
            if response.status_code == 200:
                threshold_impact = response.json()
                logger.info("Pacer threshold impact endpoint working")
                if 'results' in threshold_impact and 'total_samples' in threshold_impact['results']:
                    logger.info(f"  - Analyzed {threshold_impact['results']['total_samples']} samples")
            else:
                logger.warning("Pacer threshold impact endpoint not available")

            logger.info("‚úÖ API Server tests passed")
            return True

        except requests.exceptions.ConnectionError:
            logger.error("‚ùå Could not connect to API server")
            return False
        except Exception as e:
            logger.error(f"‚ùå API Server test failed: {e}")
            return False

    def test_advanced_analytics(self) -> bool:
        """Test the advanced analytics module"""
        logger.info("Testing Advanced Analytics...")

        try:
            from advanced_analytics import EmbeddingAnalytics, run_comprehensive_analysis

            # Test analytics initialization
            analytics = EmbeddingAnalytics("vector_db", "rmsai_metadata.db")
            logger.info("Analytics module initialized")

            # Test loading embeddings (may fail if no data)
            try:
                embeddings, metadata = analytics.load_embeddings_with_metadata()
                if len(embeddings) > 0:
                    logger.info(f"Loaded {len(embeddings)} embeddings successfully")

                    # Test clustering
                    clusters = analytics.discover_embedding_clusters()
                    if 'kmeans' in clusters:
                        logger.info(f"K-means clustering: {clusters['kmeans']['n_clusters']} clusters")

                    # Test anomaly detection
                    anomalies = analytics.detect_anomalous_patterns()
                    if 'isolation_forest' in anomalies:
                        logger.info(f"Isolation Forest: {anomalies['isolation_forest']['n_anomalies']} anomalies")

                    # Test temporal analysis
                    temporal = analytics.temporal_pattern_analysis()
                    if 'hourly_patterns' in temporal:
                        logger.info("Temporal analysis completed")

                    # Test similarity network
                    network = analytics.generate_similarity_network()
                    if 'n_similarity_groups' in network:
                        logger.info(f"Similarity network: {network['n_similarity_groups']} groups")

                else:
                    logger.warning("No embeddings available for full testing")

            except Exception as e:
                logger.warning(f"Analytics testing limited due to data availability: {e}")

            # Test comprehensive analysis runner
            logger.info("Testing comprehensive analysis runner...")
            results = run_comprehensive_analysis(output_dir="test_analytics_output")

            if 'error' not in results:
                logger.info("Comprehensive analysis completed successfully")
            else:
                logger.warning(f"Comprehensive analysis had issues: {results['error']}")

            logger.info("‚úÖ Advanced Analytics tests passed")
            return True

        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies for analytics: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Advanced Analytics test failed: {e}")
            return False

    def test_adaptive_thresholds(self) -> bool:
        """Test the adaptive thresholds module"""
        logger.info("Testing Adaptive Thresholds...")

        try:
            from adaptive_thresholds import AdaptiveThresholdManager, run_threshold_optimization

            # Test threshold manager initialization
            threshold_manager = AdaptiveThresholdManager("rmsai_metadata.db")
            logger.info("Threshold manager initialized")

            # Test initial thresholds
            thresholds = threshold_manager.get_all_thresholds()
            logger.info(f"Initial thresholds loaded for {len(thresholds)} conditions")

            # Test threshold calculation (may have limited data)
            try:
                optimal_thresholds = threshold_manager.calculate_optimal_thresholds()
                if optimal_thresholds:
                    logger.info(f"Calculated optimal thresholds for {len(optimal_thresholds)} conditions")
                    for condition, data in optimal_thresholds.items():
                        logger.info(f"  {condition}: {data['threshold']:.4f} (confidence: {data['confidence']:.3f})")
                else:
                    logger.warning("No optimal thresholds calculated (insufficient data)")

            except Exception as e:
                logger.warning(f"Threshold calculation limited: {e}")

            # Test performance evaluation
            try:
                performance = threshold_manager.evaluate_current_performance()
                if performance:
                    logger.info(f"Performance evaluation completed for {len(performance)} conditions")
                    avg_f1 = sum(p['f1_score'] for p in performance.values()) / len(performance)
                    logger.info(f"Average F1 score: {avg_f1:.3f}")
                else:
                    logger.warning("No performance data available")

            except Exception as e:
                logger.warning(f"Performance evaluation limited: {e}")

            # Test update trigger
            should_update = threshold_manager.should_update_thresholds()
            logger.info(f"Update trigger check: {should_update}")

            # Test export/import
            export_path = "test_thresholds_export.json"
            threshold_manager.export_threshold_config(export_path)
            if Path(export_path).exists():
                logger.info("Threshold configuration export successful")

                # Test import
                threshold_manager.import_threshold_config(export_path)
                logger.info("Threshold configuration import successful")

            # Test optimization runner
            logger.info("Testing threshold optimization runner...")
            results = run_threshold_optimization(output_file="test_threshold_results.json")

            if 'error' not in results:
                logger.info("Threshold optimization completed successfully")
            else:
                logger.warning(f"Threshold optimization had issues: {results['error']}")

            logger.info("‚úÖ Adaptive Thresholds tests passed")
            return True

        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies for thresholds: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Adaptive Thresholds test failed: {e}")
            return False

    def test_dashboard(self) -> bool:
        """Test the monitoring dashboard"""
        logger.info("Testing Monitoring Dashboard...")

        try:
            # Import dashboard module to check dependencies
            import streamlit as st
            import plotly.express as px
            import plotly.graph_objects as go
            logger.info("Dashboard dependencies available")

            # Test dashboard class initialization
            from dashboard import RMSAIDashboard
            dashboard = RMSAIDashboard()
            logger.info("Dashboard class initialized")

            # Test data loading
            try:
                chunks_df, files_df = dashboard.load_data(use_cache=False)
                logger.info(f"Data loading successful: {len(chunks_df)} chunks, {len(files_df)} files")
            except Exception as e:
                logger.warning(f"Data loading limited: {e}")

            # Test API stats retrieval
            try:
                api_stats = dashboard.get_api_stats()
                if api_stats:
                    logger.info("API stats retrieval successful")
                else:
                    logger.warning("API stats not available (API server not running)")
            except Exception as e:
                logger.warning(f"API stats retrieval failed: {e}")

            # Test Patient Analysis features
            try:
                # Test event condition extraction method
                if hasattr(dashboard, '_get_ground_truth_conditions'):
                    logger.info("‚úì Event condition extraction method available")

                # Test event report data extraction method
                if hasattr(dashboard, '_extract_event_report_data'):
                    logger.info("‚úì Event report data extraction method available")

                # Test PDF generation method
                if hasattr(dashboard, '_generate_pdf_report'):
                    logger.info("‚úì PDF report generation method available")

                # Test data quality scoring method
                if hasattr(dashboard, '_calculate_data_quality_score'):
                    logger.info("‚úì Data quality scoring method available")

                # Test lead configuration method
                if hasattr(dashboard, '_get_lead_configuration'):
                    logger.info("‚úì Lead configuration method available")

                # Test PDF dependencies
                try:
                    from reportlab.lib.pagesizes import A4
                    from reportlab.platypus import SimpleDocTemplate
                    logger.info("‚úì PDF generation dependencies available")
                except ImportError:
                    logger.warning("‚ö† PDF generation dependencies missing (reportlab)")

                # Test HDF5 dependencies
                try:
                    import h5py
                    logger.info("‚úì HDF5 processing dependencies available")
                except ImportError:
                    logger.warning("‚ö† HDF5 processing dependencies missing (h5py)")

                logger.info("‚úì Patient Analysis features structure verified")

            except Exception as e:
                logger.warning(f"Patient Analysis features test failed: {e}")

            # Note: Cannot fully test Streamlit dashboard without running it
            logger.info("Dashboard module structure verified")
            logger.info("To test dashboard fully, run: streamlit run dashboard.py")
            logger.info("To test Patient Analysis, navigate to 'üë• Patient Analysis' tab")

            logger.info("‚úÖ Monitoring Dashboard tests passed")
            return True

        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies for dashboard: {e}")
            logger.info("Install with: pip install streamlit plotly reportlab h5py")
            return False
        except Exception as e:
            logger.error(f"‚ùå Monitoring Dashboard test failed: {e}")
            return False

    def test_patient_analysis(self) -> bool:
        """Test Patient Analysis functionality"""
        logger.info("Testing Patient Analysis functionality...")

        try:
            # Import required dependencies
            from dashboard import RMSAIDashboard
            import pandas as pd
            import sqlite3
            import numpy as np

            dashboard = RMSAIDashboard()
            logger.info("Dashboard initialized for Patient Analysis testing")

            # Test chunk ID parsing logic
            test_chunk_ids = [
                "chunk_10011_0",
                "chunk_10011_1120",
                "chunk_10011_2100"
            ]

            for chunk_id in test_chunk_ids:
                try:
                    # Test chunk number extraction
                    parts = chunk_id.split('_')
                    if len(parts) >= 3:
                        chunk_num = int(parts[-1])
                        # Test offset calculation (200Hz, 12-second strips)
                        offset_seconds = chunk_num / 200
                        logger.info(f"‚úì Chunk {chunk_id} ‚Üí offset {offset_seconds:.1f}s")
                    else:
                        logger.warning(f"‚ö† Unexpected chunk format: {chunk_id}")
                except Exception as e:
                    logger.error(f"‚ùå Chunk parsing failed for {chunk_id}: {e}")

            # Test data quality scoring
            try:
                # Create synthetic ECG data for testing
                sampling_rate = 200
                duration = 12  # seconds
                test_signal = np.random.randn(sampling_rate * duration) * 0.1

                if hasattr(dashboard, '_calculate_data_quality_score'):
                    quality_score = dashboard._calculate_data_quality_score(test_signal, sampling_rate)
                    logger.info(f"‚úì Data quality scoring: {quality_score:.1f}/100")
                else:
                    logger.warning("‚ö† Data quality scoring method not found")
            except Exception as e:
                logger.warning(f"‚ö† Data quality scoring test failed: {e}")

            # Test PDF generation structure
            try:
                if hasattr(dashboard, '_generate_pdf_report'):
                    # Test with minimal data
                    test_report_data = {
                        'device_info': {'device_id': 'TEST_001'},
                        'event_info': {'uuid': 'test_event'},
                        'ecg_data': {'leads': {'I': {'length': 2400, 'sampling_rate': 200}}},
                        'vitals': {}
                    }
                    test_ai_data = {
                        'ai_verdict': 'Normal',
                        'event_condition': 'Normal',
                        'error_score': 0.5,
                        'timestamp': '2025-01-01 12:00:00'
                    }

                    # Note: Not actually generating PDF to avoid file creation in tests
                    logger.info("‚úì PDF generation method structure verified")
                else:
                    logger.warning("‚ö† PDF generation method not found")
            except Exception as e:
                logger.warning(f"‚ö† PDF generation test failed: {e}")

            # Test event condition extraction structure
            try:
                if hasattr(dashboard, '_get_ground_truth_conditions'):
                    logger.info("‚úì Event condition extraction method available")
                else:
                    logger.warning("‚ö† Event condition extraction method not found")
            except Exception as e:
                logger.warning(f"‚ö† Event condition extraction test failed: {e}")

            logger.info("‚úÖ Patient Analysis functionality tests completed")
            return True

        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies for Patient Analysis: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Patient Analysis test failed: {e}")
            return False

    def test_integration(self) -> bool:
        """Test integration between components"""
        logger.info("Testing Integration...")

        try:
            # Test API + Analytics integration
            if 'api' in self.processes:
                try:
                    # Test similarity search endpoint (requires vector data)
                    response = requests.post(
                        "http://localhost:8000/api/v1/search/similar",
                        json={"chunk_id": "test_chunk", "n_results": 5},
                        timeout=5
                    )
                    # 404 is expected if chunk doesn't exist
                    if response.status_code in [200, 404, 503]:
                        logger.info("Similarity search endpoint accessible")
                    else:
                        logger.warning(f"Similarity search returned: {response.status_code}")

                except Exception as e:
                    logger.warning(f"Similarity search test limited: {e}")

            # Test threshold + API integration
            try:
                from adaptive_thresholds import AdaptiveThresholdManager
                threshold_manager = AdaptiveThresholdManager("rmsai_metadata.db")

                # Get threshold for a condition
                threshold = threshold_manager.get_threshold_for_condition("Normal")
                logger.info(f"Threshold integration working: Normal = {threshold}")

            except Exception as e:
                logger.warning(f"Threshold integration test limited: {e}")

            logger.info("‚úÖ Integration tests completed")
            return True

        except Exception as e:
            logger.error(f"‚ùå Integration test failed: {e}")
            return False

    def cleanup(self):
        """Clean up test processes and files"""
        logger.info("Cleaning up test environment...")

        # Stop API server
        if 'api' in self.processes:
            try:
                self.processes['api'].terminate()
                self.processes['api'].wait(timeout=5)
                logger.info("API server stopped")
            except Exception as e:
                logger.warning(f"Error stopping API server: {e}")

        # Clean up test files
        test_files = [
            "test_thresholds_export.json",
            "test_threshold_results.json",
            "test_analytics_output"
        ]

        for file_path in test_files:
            try:
                path = Path(file_path)
                if path.is_file():
                    path.unlink()
                elif path.is_dir():
                    import shutil
                    shutil.rmtree(path)
                logger.info(f"Cleaned up {file_path}")
            except Exception as e:
                logger.warning(f"Could not clean up {file_path}: {e}")

    def test_pacer_functionality(self) -> bool:
        """Test pacer_info and pacer_offset functionality in HDF5 files"""
        logger.info("Testing Pacer Functionality...")

        try:
            # Generate a test HDF5 file with pacer data
            logger.info("Generating test HDF5 file with pacer data...")

            import subprocess
            import os

            # Generate a small test file
            result = subprocess.run([
                sys.executable, "rmsai_sim_hdf5_data.py", "2", "--patient-id", "TEST_PACER"
            ], capture_output=True, text=True, timeout=30)

            if result.returncode != 0:
                logger.error(f"Failed to generate test HDF5 file: {result.stderr}")
                return False

            # Find the generated file
            test_file = None
            for file in os.listdir("data"):
                if file.startswith("TEST_PACER") and file.endswith(".h5"):
                    test_file = os.path.join("data", file)
                    break

            if not test_file:
                logger.error("Test HDF5 file not found")
                return False

            logger.info(f"Generated test file: {test_file}")

            # Test HDF5 access with pacer data
            logger.info("Testing HDF5 access with pacer analysis...")

            # Import the h5access module to test pacer functionality
            sys.path.insert(0, '.')
            from rmsai_h5access import analyze_pacer_data

            import h5py

            # Open the test file and validate pacer data
            with h5py.File(test_file, 'r') as f:
                events = [key for key in f.keys() if key.startswith('event_')]

                if not events:
                    logger.error("No events found in test file")
                    return False

                first_event = f[events[0]]

                # Test pacer_info presence
                if 'pacer_info' not in first_event['ecg']:
                    logger.error("pacer_info not found in ECG data")
                    return False

                pacer_info = first_event['ecg']['pacer_info'][()]
                logger.info(f"Found pacer_info: 0x{pacer_info:08X}")

                # Test pacer_offset presence
                if 'pacer_offset' not in first_event['ecg']:
                    logger.error("pacer_offset not found in ECG data")
                    return False

                pacer_offset = first_event['ecg']['pacer_offset'][()]
                logger.info(f"Found pacer_offset: {pacer_offset} samples")

                # Validate pacer_offset range
                if not (0 <= pacer_offset <= 2400):
                    logger.error(f"pacer_offset out of valid range: {pacer_offset}")
                    return False

                # Test comprehensive pacer analysis
                pacer_data = analyze_pacer_data(first_event)

                if 'info' not in pacer_data:
                    logger.error("Pacer info analysis failed")
                    return False

                if 'timing' not in pacer_data:
                    logger.error("Pacer timing analysis failed")
                    return False

                pacer_type = pacer_data['info']['type']
                timing_category = pacer_data['timing']['timing_category']
                time_offset = pacer_data['timing']['offset_seconds']

                logger.info(f"Pacer analysis results:")
                logger.info(f"  - Type: {pacer_type} ({pacer_data['info']['type_name']})")
                logger.info(f"  - Timing: {timing_category} ({time_offset:.3f}s)")

                if 'signal_analysis' in pacer_data:
                    signal_at_spike = pacer_data['signal_analysis']['pacer_amplitude_at_spike']
                    logger.info(f"  - Signal at pacer spike: {signal_at_spike:.3f} mV")

            # Test file structure validation
            logger.info("Testing file structure validation...")
            result = subprocess.run([
                sys.executable, "rmsai_h5access.py", test_file
            ], capture_output=True, text=True, timeout=30)

            if result.returncode != 0:
                logger.error(f"H5 access validation failed: {result.stderr}")
                return False

            # Check if pacer data is mentioned in the output
            if "Pacer Information" not in result.stdout:
                logger.error("Pacer information not displayed in h5access output")
                return False

            if "Pacer Timing" not in result.stdout:
                logger.error("Pacer timing not displayed in h5access output")
                return False

            logger.info("Pacer data correctly displayed in h5access output")

            # Clean up test file
            try:
                os.remove(test_file)
                logger.info("Test file cleaned up")
            except:
                pass

            logger.info("‚úÖ Pacer functionality tests passed")
            return True

        except Exception as e:
            logger.error(f"‚ùå Pacer functionality test failed: {e}")
            return False

    def test_lstm_processor(self) -> bool:
        """Test LSTM autoencoder processor functionality"""
        logger.info("Testing LSTM Processor...")

        try:
            # Generate test data for processor
            logger.info("Generating test data for LSTM processor...")

            import subprocess
            import os
            import sqlite3
            import time

            # Generate a test HDF5 file
            result = subprocess.run([
                sys.executable, "rmsai_sim_hdf5_data.py", "1", "--patient-id", "PROC_TEST"
            ], capture_output=True, text=True, timeout=30)

            if result.returncode != 0:
                logger.error(f"Failed to generate test data: {result.stderr}")
                return False

            # Find the generated file
            test_file = None
            for file in os.listdir("data"):
                if file.startswith("PROC_TEST") and file.endswith(".h5"):
                    test_file = os.path.join("data", file)
                    break

            if not test_file:
                logger.error("Test HDF5 file not found")
                return False

            logger.info(f"Generated test file: {test_file}")

            # Test processor components individually
            logger.info("Testing processor configuration...")

            # Import processor components
            sys.path.insert(0, '.')
            from rmsai_lstm_autoencoder_proc import RMSAIConfig, ECGChunkProcessor

            # Test configuration
            config = RMSAIConfig()
            logger.info(f"Config - seq_len: {config.seq_len}, embedding_dim: {config.embedding_dim}")

            if config.seq_len != 140:
                logger.error(f"Expected seq_len=140, got {config.seq_len}")
                return False

            if config.embedding_dim != 128:
                logger.error(f"Expected embedding_dim=128, got {config.embedding_dim}")
                return False

            # Test chunk processor initialization
            logger.info("Testing chunk processor initialization...")
            try:
                chunk_processor = ECGChunkProcessor(config)
                logger.info("Chunk processor initialized successfully")
            except Exception as e:
                logger.warning(f"Chunk processor initialization failed: {e}")
                logger.info("This is expected if model.pth doesn't exist")

            # Test chunking logic by reading HDF5 data
            logger.info("Testing chunking logic...")

            import h5py
            import numpy as np

            with h5py.File(test_file, 'r') as f:
                events = [key for key in f.keys() if key.startswith('event_')]
                if not events:
                    logger.error("No events found in test file")
                    return False

                event = f[events[0]]
                ecg_data = event['ecg']['ECG1'][:]

                logger.info(f"ECG data length: {len(ecg_data)} samples")

                # Test chunking parameters (updated for 100% coverage)
                chunk_size = 140
                step_size = 141  # Optimized for 100% coverage (99.8%)

                chunks_generated = 0
                for chunk_start in range(0, len(ecg_data) - chunk_size + 1, step_size):
                    chunk = ecg_data[chunk_start:chunk_start + chunk_size]

                    if len(chunk) != chunk_size:
                        logger.error(f"Chunk size mismatch: expected {chunk_size}, got {len(chunk)}")
                        return False

                    chunks_generated += 1
                    if chunks_generated >= 10:  # Test first 10 chunks
                        break

                logger.info(f"Successfully generated {chunks_generated} chunks from ECG data")

                # Verify improved coverage with new step size
                total_samples = len(ecg_data)
                last_chunk_end = chunks_generated * step_size + chunk_size - step_size
                coverage_pct = (last_chunk_end / total_samples) * 100 if total_samples > 0 else 0
                logger.info(f"ECG coverage: {coverage_pct:.1f}% ({last_chunk_end}/{total_samples} samples)")

                # Expect significantly improved coverage (should be ~99.8% vs old 32%)
                if coverage_pct < 90:
                    logger.warning(f"Coverage may be lower than expected: {coverage_pct:.1f}%")
                else:
                    logger.info(f"‚úÖ Excellent coverage achieved: {coverage_pct:.1f}%")

                # Test preprocessing logic
                logger.info("Testing preprocessing logic...")

                # Simulate preprocessing
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()

                test_chunk = ecg_data[:chunk_size]
                chunk_normalized = scaler.fit_transform(test_chunk.reshape(-1, 1)).flatten()

                if len(chunk_normalized) != chunk_size:
                    logger.error(f"Normalization failed: expected {chunk_size}, got {len(chunk_normalized)}")
                    return False

                logger.info("Preprocessing logic working correctly")

            # Test database integration (if processor has run)
            logger.info("Testing database integration...")

            if os.path.exists("rmsai_metadata.db"):
                with sqlite3.connect("rmsai_metadata.db") as conn:
                    cursor = conn.cursor()

                    # Check if chunks table exists
                    cursor.execute("""
                        SELECT name FROM sqlite_master
                        WHERE type='table' AND name='chunks'
                    """)
                    if cursor.fetchone():
                        # Check for recent chunks
                        cursor.execute("""
                            SELECT COUNT(*) FROM chunks
                            WHERE processing_timestamp > datetime('now', '-1 hour')
                        """)
                        recent_chunks = cursor.fetchone()[0]
                        logger.info(f"Found {recent_chunks} recently processed chunks")

                        # Check for pacer data (if column exists)
                        try:
                            cursor.execute("""
                                SELECT COUNT(*) FROM chunks
                                WHERE pacer_offset IS NOT NULL
                            """)
                            pacer_chunks = cursor.fetchone()[0]
                            logger.info(f"Found {pacer_chunks} chunks with pacer data")
                        except sqlite3.OperationalError as e:
                            if "no such column: pacer_offset" in str(e):
                                logger.info("Database schema predates pacer_offset feature")
                            else:
                                raise
                    else:
                        logger.info("Chunks table not found (processor may not have run)")
            else:
                logger.info("Metadata database not found (processor may not have run)")

            # Test ChromaDB integration
            logger.info("Testing ChromaDB integration...")

            try:
                import chromadb
                if os.path.exists("vector_db"):
                    client = chromadb.PersistentClient(path="vector_db")
                    try:
                        collection = client.get_collection("rmsai_ecg_embeddings")
                        count = collection.count()
                        logger.info(f"ChromaDB collection has {count} embeddings")

                        if count > 0:
                            # Test embedding dimension
                            result = collection.peek(limit=1)
                            embeddings = result.get('embeddings', [])
                            if embeddings and len(embeddings) > 0:
                                embedding_dim = len(embeddings[0])
                                logger.info(f"Embedding dimension: {embedding_dim}")
                                if embedding_dim != 128:
                                    logger.warning(f"Expected 128-dim embeddings, found {embedding_dim}")

                    except Exception as e:
                        logger.info(f"ChromaDB collection not found: {e}")
                else:
                    logger.info("ChromaDB directory not found")
            except ImportError:
                logger.warning("ChromaDB not available for testing")

            # Clean up test file
            try:
                os.remove(test_file)
                logger.info("Test file cleaned up")
            except:
                pass

            logger.info("‚úÖ LSTM Processor tests passed")
            return True

        except Exception as e:
            logger.error(f"‚ùå LSTM Processor test failed: {e}")
            return False

    def test_analysis_tools(self) -> bool:
        """Test analysis tools in analysis/ directory"""
        logger.info("Testing Analysis Tools...")

        try:
            import os
            import sys
            import subprocess

            # Check if analysis directory exists
            analysis_dir = "analysis"
            if not os.path.exists(analysis_dir):
                logger.error(f"Analysis directory not found: {analysis_dir}")
                return False

            logger.info(f"Found analysis directory: {analysis_dir}")

            # Expected analysis tools
            expected_tools = [
                "condition_comparison_tool.py",
                "chunking_analysis.py",
                "full_coverage_analysis.py",
                "coverage_optimization.py"
            ]

            missing_tools = []
            for tool in expected_tools:
                tool_path = os.path.join(analysis_dir, tool)
                if os.path.exists(tool_path):
                    logger.info(f"‚úÖ Found: {tool}")
                else:
                    missing_tools.append(tool)
                    logger.warning(f"‚ùå Missing: {tool}")

            if missing_tools:
                logger.warning(f"Missing analysis tools: {missing_tools}")
            else:
                logger.info("‚úÖ All expected analysis tools found")

            # Test condition comparison tool
            try:
                logger.info("Testing condition comparison tool...")
                result = subprocess.run([
                    sys.executable, os.path.join(analysis_dir, "condition_comparison_tool.py"),
                    "--format", "table", "--limit", "5"
                ], capture_output=True, text=True, timeout=30)

                if result.returncode == 0:
                    logger.info("‚úÖ Condition comparison tool working")
                    if "Input Condition" in result.stdout:
                        logger.info("Tool output contains expected headers")
                else:
                    logger.warning(f"Condition comparison tool returned error: {result.stderr}")
            except Exception as e:
                logger.warning(f"Could not test condition comparison tool: {e}")

            # Test chunking analysis tool
            try:
                logger.info("Testing chunking analysis tool...")
                result = subprocess.run([
                    sys.executable, os.path.join(analysis_dir, "chunking_analysis.py")
                ], capture_output=True, text=True, timeout=30)

                if result.returncode == 0:
                    logger.info("‚úÖ Chunking analysis tool working")
                    # Check for coverage information in output
                    if "coverage" in result.stdout.lower():
                        logger.info("Tool output contains coverage analysis")
                else:
                    logger.warning(f"Chunking analysis tool returned error: {result.stderr}")
            except Exception as e:
                logger.warning(f"Could not test chunking analysis tool: {e}")

            # Test full coverage analysis tool
            try:
                logger.info("Testing full coverage analysis tool...")
                result = subprocess.run([
                    sys.executable, os.path.join(analysis_dir, "full_coverage_analysis.py")
                ], capture_output=True, text=True, timeout=30)

                if result.returncode == 0:
                    logger.info("‚úÖ Full coverage analysis tool working")
                    # Check for 100% coverage mention
                    if "100%" in result.stdout:
                        logger.info("Tool mentions 100% coverage optimization")
                else:
                    logger.warning(f"Full coverage analysis tool returned error: {result.stderr}")
            except Exception as e:
                logger.warning(f"Could not test full coverage analysis tool: {e}")

            logger.info("‚úÖ Analysis tools tests completed")
            return True

        except Exception as e:
            logger.error(f"‚ùå Analysis tools test failed: {e}")
            return False

    def test_threshold_classification(self) -> bool:
        """Test threshold values and heart rate-based classification"""
        logger.info("Testing Threshold Classification System...")

        try:
            # Import and run the comprehensive threshold tests
            from test_threshold_classification import ThresholdClassificationTester

            tester = ThresholdClassificationTester()
            results = tester.run_all_tests()

            # Check if all tests passed
            all_passed = all(results.values())

            if all_passed:
                logger.info("‚úÖ All threshold classification tests passed")
            else:
                failed_tests = [name for name, passed in results.items() if not passed]
                logger.error(f"‚ùå Some threshold tests failed: {failed_tests}")

            return all_passed

        except ImportError as e:
            logger.error(f"‚ùå Could not import threshold classification tests: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Threshold classification test failed: {e}")
            return False

    def run_tests(self, module: str = None) -> Dict[str, bool]:
        """Run all tests or specific module tests"""
        logger.info("Starting RMSAI Improvements Test Suite")
        logger.info("=" * 60)

        tests_to_run = {
            'api': self.test_api_server,
            'analytics': self.test_advanced_analytics,
            'thresholds': self.test_adaptive_thresholds,
            'classification': self.test_threshold_classification,
            'dashboard': self.test_dashboard,
            'patient_analysis': self.test_patient_analysis,
            'pacer': self.test_pacer_functionality,
            'processor': self.test_lstm_processor,
            'analysis': self.test_analysis_tools
        }

        if module and module in tests_to_run:
            tests_to_run = {module: tests_to_run[module]}

        results = {}

        try:
            for test_name, test_func in tests_to_run.items():
                logger.info(f"\nüìã Testing {test_name.upper()}...")
                try:
                    results[test_name] = test_func()
                except Exception as e:
                    logger.error(f"Test {test_name} failed with exception: {e}")
                    results[test_name] = False

            # Run integration tests if testing multiple modules
            if len(tests_to_run) > 1:
                logger.info(f"\nüîó Testing INTEGRATION...")
                results['integration'] = self.test_integration()

        finally:
            self.cleanup()

        # Print summary
        logger.info("\n" + "=" * 60)
        logger.info("TEST RESULTS SUMMARY")
        logger.info("=" * 60)

        passed = 0
        total = len(results)

        for test_name, result in results.items():
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            logger.info(f"{test_name.upper():15s}: {status}")
            if result:
                passed += 1

        logger.info(f"\nOverall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")

        if passed == total:
            logger.info("üéâ All tests passed!")
        else:
            logger.warning("‚ö†Ô∏è Some tests failed - check logs above")

        return results

def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Test RMSAI improvements")
    parser.add_argument(
        'module',
        nargs='?',
        choices=['api', 'analytics', 'thresholds', 'classification', 'dashboard', 'patient_analysis', 'pacer', 'processor', 'analysis'],
        help='Specific module to test (default: all)'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Verbose output'
    )

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    tester = ImprovementTester()
    results = tester.run_tests(args.module)

    # Exit with error code if tests failed
    if not all(results.values()):
        sys.exit(1)

if __name__ == "__main__":
    main()